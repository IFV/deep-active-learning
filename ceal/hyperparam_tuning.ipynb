{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#from __future__ import division\n",
    "\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gzip\n",
    "from scipy.misc import imsave\n",
    "import time\n",
    "import scipy.ndimage\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Dropout\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "from keras import losses\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    # image dimension (array)\n",
    "    img_rows = 512\n",
    "    img_cols = 512\n",
    "\n",
    "    batch_size = 8\n",
    "    \n",
    "    def preprocessor(input_img, img_rows, img_cols):\n",
    "        \"\"\"\n",
    "        Resize input images to constants sizes\n",
    "        :param input_img: numpy array of images\n",
    "        :return: numpy array of preprocessed images\n",
    "        \"\"\"\n",
    "        output_img = np.ndarray((input_img.shape[0], input_img.shape[1], img_rows, img_cols), dtype=np.uint8)\n",
    "\n",
    "        for i in range(input_img.shape[0]):\n",
    "            output_img[i, 0] = cv2.resize(input_img[i, 0], (img_cols, img_rows), interpolation=cv2.INTER_CUBIC)\n",
    "        return output_img\n",
    "\n",
    "\n",
    "    def load_data(path_img, path_mask, img_rows, img_cols):\n",
    "        \"\"\"\n",
    "        Load data from project path\n",
    "        :return: [X, y] numpy arrays containing the [training, validation, test] data and their respective masks.\n",
    "        \"\"\"\n",
    "        print(\"\\nLoading data...\\n\")\n",
    "        X = np.load(path_img)\n",
    "        y = np.load(path_mask)\n",
    "\n",
    "        X = preprocessor(X, img_rows, img_cols)\n",
    "        y = preprocessor(y, img_rows, img_cols)\n",
    "\n",
    "        X = X.astype('float32')\n",
    "\n",
    "        mean = np.mean(X)  # mean for data centering\n",
    "        std = np.std(X)  # std for data normalization\n",
    "\n",
    "        X -= mean\n",
    "        X /= std\n",
    "\n",
    "        y = y.astype('float32')\n",
    "        y /= 255.  # scale masks to [0, 1]\n",
    "        return X, y\n",
    "    \n",
    "    data_path = '/home/malub_local/data/'\n",
    "\n",
    "    path_img_train = data_path + 'images_69.npy'\n",
    "    path_mask_train = data_path + 'masks_69.npy'\n",
    "\n",
    "    path_img_valid = data_path + 'images_valid_29.npy'\n",
    "    path_mask_valid = data_path + 'masks_valid_29.npy'\n",
    "\n",
    "    path_img_test = data_path + 'images_test_5.npy'\n",
    "    path_mask_test= data_path + 'masks_test_5.npy'\n",
    "\n",
    "    \n",
    "    X_train, y_train = load_data(path_img_train, path_mask_train, img_rows, img_cols )\n",
    "    X_valid, y_valid = load_data(path_img_valid, path_mask_valid, img_rows, img_cols)\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = True\n",
    "learning_rate = 1e-3\n",
    "nb_initial_epochs = 10\n",
    "decay_rate = learning_rate / nb_initial_epochs\n",
    "\n",
    "\n",
    "def create_model(X_train, y_train, X_valid, y_valid):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "        \n",
    "    \"\"\"\n",
    "    K.set_image_dim_ordering('th') \n",
    "    dropout = True\n",
    "    learning_rate = 1e-3\n",
    "    nb_initial_epochs = 10\n",
    "    decay_rate = learning_rate / nb_initial_epochs\n",
    "    \n",
    "    #### loss and metrics #####\n",
    "    def dice_coef(y_true, y_pred, smooth = 1.):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "    \"\"\"def dice_coef(y_true, y_pred, smooth=1):\n",
    "\n",
    "        intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "        return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "            # Calculate the binary crossentropy\n",
    "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "            # Apply the weights\n",
    "        weight_vector = y_true * 0.30 + (1. - y_true) * 0.70\n",
    "        weighted_b_ce = weight_vector * b_ce\n",
    "\n",
    "            # Return the mean error\n",
    "        return K.mean(weighted_b_ce)\n",
    "\n",
    "    \n",
    "    inputs = Input((1, img_rows, img_cols))\n",
    "    #conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "    batch1 = BatchNormalization(axis=1)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(batch1)\n",
    "\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(conv2)\n",
    "    batch2 = BatchNormalization(axis=1)(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(batch2)\n",
    "\n",
    "   \n",
    "    conv3 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(conv3)\n",
    "    batch3 = BatchNormalization(axis=1)(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(batch3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(conv4)\n",
    "    batch4 = BatchNormalization(axis=1)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(batch4)\n",
    "    \n",
    "    if dropout:\n",
    "        pool4 = Dropout({{choice([0.3, 0.5, 0.7])}})(pool4)\n",
    "   \n",
    "    conv5 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(conv5)\n",
    "\n",
    "    if dropout:\n",
    "        conv5 = Dropout({{choice([0.3, 0.5, 0.7])}})(conv5)    \n",
    "\n",
    "    up6_interm = UpSampling2D(size=(2, 2))(conv5)\n",
    "    \n",
    "    up6 = concatenate([up6_interm, conv4], axis=1)\n",
    "\n",
    "    conv6 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(conv6)\n",
    "    batch6 = BatchNormalization(axis=1)(conv6)\n",
    "\n",
    "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=1)\n",
    "    \n",
    "    if dropout:\n",
    "        up7 = Dropout({{choice([0.3, 0.5])}})(up7)  \n",
    "        \n",
    "    conv7 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(conv7)\n",
    "    batch7 = BatchNormalization(axis=1)(conv7)\n",
    "\n",
    "    up8 = concatenate([UpSampling2D(size=(2, 2))(batch7), conv2], axis=1)\n",
    "\n",
    "    conv8 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(conv8)\n",
    "    batch8 = BatchNormalization(axis=1)(conv8)\n",
    "\n",
    "    up9 = concatenate([UpSampling2D(size=(2, 2))(batch8), conv1], axis=1)\n",
    "\n",
    "    conv9 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(conv9)\n",
    "    batch9 = BatchNormalization(axis=1)(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation=\"sigmoid\")(batch9)\n",
    "\n",
    "    model = Model(outputs=conv10, inputs=inputs)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr = learning_rate, decay=decay_rate), loss=weighted_binary_crossentropy, \n",
    "                  metrics = [dice_coef])\n",
    "    \n",
    "    \n",
    "    model.fit(X_train, y_train, \n",
    "                        validation_data = (X_valid, y_valid),\n",
    "                        batch_size = 8,\n",
    "                        epochs = 10, \n",
    "                        verbose = 1,\n",
    "                        shuffle = True\n",
    "                        ) \n",
    "                                  #validation_steps= 5,\n",
    "                                  #callbacks = [tensorboard])\n",
    "\n",
    "\n",
    "    score, dice = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "    print('Test accuracy:', dice)\n",
    "    return {'loss': -dice, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import cv2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import gzip\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from scipy.misc import imsave\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import scipy.ndimage\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import *\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.merge import concatenate\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import *\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import *\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import losses\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.image import ImageDataGenerator\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dropout': hp.choice('Dropout', [0.3, 0.5, 0.7]),\n",
      "        'Dropout_1': hp.choice('Dropout_1', [0.3, 0.5, 0.7]),\n",
      "        'Dropout_2': hp.choice('Dropout_2', [0.3, 0.5]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: # image dimension (array)\n",
      "   3: img_rows = 512\n",
      "   4: img_cols = 512\n",
      "   5: \n",
      "   6: batch_size = 8\n",
      "   7: \n",
      "   8: def preprocessor(input_img, img_rows, img_cols):\n",
      "   9:     \"\"\"\n",
      "  10:     Resize input images to constants sizes\n",
      "  11:     :param input_img: numpy array of images\n",
      "  12:     :return: numpy array of preprocessed images\n",
      "  13:     \"\"\"\n",
      "  14:     output_img = np.ndarray((input_img.shape[0], input_img.shape[1], img_rows, img_cols), dtype=np.uint8)\n",
      "  15: \n",
      "  16:     for i in range(input_img.shape[0]):\n",
      "  17:         output_img[i, 0] = cv2.resize(input_img[i, 0], (img_cols, img_rows), interpolation=cv2.INTER_CUBIC)\n",
      "  18:     return output_img\n",
      "  19: \n",
      "  20: \n",
      "  21: def load_data(path_img, path_mask, img_rows, img_cols):\n",
      "  22:     \"\"\"\n",
      "  23:     Load data from project path\n",
      "  24:     :return: [X, y] numpy arrays containing the [training, validation, test] data and their respective masks.\n",
      "  25:     \"\"\"\n",
      "  26:     print(\"\\nLoading data...\\n\")\n",
      "  27:     X = np.load(path_img)\n",
      "  28:     y = np.load(path_mask)\n",
      "  29: \n",
      "  30:     X = preprocessor(X, img_rows, img_cols)\n",
      "  31:     y = preprocessor(y, img_rows, img_cols)\n",
      "  32: \n",
      "  33:     X = X.astype('float32')\n",
      "  34: \n",
      "  35:     mean = np.mean(X)  # mean for data centering\n",
      "  36:     std = np.std(X)  # std for data normalization\n",
      "  37: \n",
      "  38:     X -= mean\n",
      "  39:     X /= std\n",
      "  40: \n",
      "  41:     y = y.astype('float32')\n",
      "  42:     y /= 255.  # scale masks to [0, 1]\n",
      "  43:     return X, y\n",
      "  44: \n",
      "  45: data_path = '/home/malub_local/data/'\n",
      "  46: \n",
      "  47: path_img_train = data_path + 'images_69.npy'\n",
      "  48: path_mask_train = data_path + 'masks_69.npy'\n",
      "  49: \n",
      "  50: path_img_valid = data_path + 'images_valid_29.npy'\n",
      "  51: path_mask_valid = data_path + 'masks_valid_29.npy'\n",
      "  52: \n",
      "  53: path_img_test = data_path + 'images_test_5.npy'\n",
      "  54: path_mask_test= data_path + 'masks_test_5.npy'\n",
      "  55: \n",
      "  56: \n",
      "  57: X_train, y_train = load_data(path_img_train, path_mask_train, img_rows, img_cols )\n",
      "  58: X_valid, y_valid = load_data(path_img_valid, path_mask_valid, img_rows, img_cols)\n",
      "  59: \n",
      "  60: \n",
      "  61: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:         \n",
      "  13:     \"\"\"\n",
      "  14:     K.set_image_dim_ordering('th') \n",
      "  15:     dropout = True\n",
      "  16:     learning_rate = 1e-3\n",
      "  17:     nb_initial_epochs = 10\n",
      "  18:     decay_rate = learning_rate / nb_initial_epochs\n",
      "  19:     \n",
      "  20:     #### loss and metrics #####\n",
      "  21:     def dice_coef(y_true, y_pred, smooth = 1.):\n",
      "  22:         y_true_f = K.flatten(y_true)\n",
      "  23:         y_pred_f = K.flatten(y_pred)\n",
      "  24:         intersection = K.sum(y_true_f * y_pred_f)\n",
      "  25:         return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
      "  26: \n",
      "  27: \n",
      "  28:     \"\"\"def dice_coef(y_true, y_pred, smooth=1):\n",
      "  29: \n",
      "  30:         intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
      "  31:         return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\"\"\"\n",
      "  32: \n",
      "  33: \n",
      "  34: \n",
      "  35:     def weighted_binary_crossentropy(y_true, y_pred):\n",
      "  36:             # Calculate the binary crossentropy\n",
      "  37:         b_ce = K.binary_crossentropy(y_true, y_pred)\n",
      "  38: \n",
      "  39:             # Apply the weights\n",
      "  40:         weight_vector = y_true * 0.30 + (1. - y_true) * 0.70\n",
      "  41:         weighted_b_ce = weight_vector * b_ce\n",
      "  42: \n",
      "  43:             # Return the mean error\n",
      "  44:         return K.mean(weighted_b_ce)\n",
      "  45: \n",
      "  46:     \n",
      "  47:     inputs = Input((1, img_rows, img_cols))\n",
      "  48:     #conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
      "  49:     conv1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n",
      "  50:     conv1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(conv1)\n",
      "  51:     batch1 = BatchNormalization(axis=1)(conv1)\n",
      "  52:     pool1 = MaxPooling2D(pool_size=(2, 2))(batch1)\n",
      "  53: \n",
      "  54: \n",
      "  55:     conv2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(pool1)\n",
      "  56:     conv2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(conv2)\n",
      "  57:     batch2 = BatchNormalization(axis=1)(conv2)\n",
      "  58:     pool2 = MaxPooling2D(pool_size=(2, 2))(batch2)\n",
      "  59: \n",
      "  60:    \n",
      "  61:     conv3 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(pool2)\n",
      "  62:     conv3 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(conv3)\n",
      "  63:     batch3 = BatchNormalization(axis=1)(conv3)\n",
      "  64:     pool3 = MaxPooling2D(pool_size=(2, 2))(batch3)\n",
      "  65: \n",
      "  66:     conv4 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(pool3)\n",
      "  67:     conv4 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(conv4)\n",
      "  68:     batch4 = BatchNormalization(axis=1)(conv4)\n",
      "  69:     pool4 = MaxPooling2D(pool_size=(2, 2))(batch4)\n",
      "  70:     \n",
      "  71:     if dropout:\n",
      "  72:         pool4 = Dropout(space['Dropout'])(pool4)\n",
      "  73:    \n",
      "  74:     conv5 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(pool4)\n",
      "  75:     conv5 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(conv5)\n",
      "  76: \n",
      "  77:     if dropout:\n",
      "  78:         conv5 = Dropout(space['Dropout_1'])(conv5)    \n",
      "  79: \n",
      "  80:     up6_interm = UpSampling2D(size=(2, 2))(conv5)\n",
      "  81:     \n",
      "  82:     up6 = concatenate([up6_interm, conv4], axis=1)\n",
      "  83: \n",
      "  84:     conv6 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(up6)\n",
      "  85:     conv6 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(conv6)\n",
      "  86:     batch6 = BatchNormalization(axis=1)(conv6)\n",
      "  87: \n",
      "  88:     up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=1)\n",
      "  89:     \n",
      "  90:     if dropout:\n",
      "  91:         up7 = Dropout(space['Dropout_2'])(up7)  \n",
      "  92:         \n",
      "  93:     conv7 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(up7)\n",
      "  94:     conv7 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(conv7)\n",
      "  95:     batch7 = BatchNormalization(axis=1)(conv7)\n",
      "  96: \n",
      "  97:     up8 = concatenate([UpSampling2D(size=(2, 2))(batch7), conv2], axis=1)\n",
      "  98: \n",
      "  99:     conv8 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(up8)\n",
      " 100:     conv8 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(conv8)\n",
      " 101:     batch8 = BatchNormalization(axis=1)(conv8)\n",
      " 102: \n",
      " 103:     up9 = concatenate([UpSampling2D(size=(2, 2))(batch8), conv1], axis=1)\n",
      " 104: \n",
      " 105:     conv9 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(up9)\n",
      " 106:     conv9 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(conv9)\n",
      " 107:     batch9 = BatchNormalization(axis=1)(conv9)\n",
      " 108: \n",
      " 109:     conv10 = Conv2D(1, (1, 1), activation=\"sigmoid\")(batch9)\n",
      " 110: \n",
      " 111:     model = Model(outputs=conv10, inputs=inputs)\n",
      " 112: \n",
      " 113:     model.compile(optimizer=Adam(lr = learning_rate, decay=decay_rate), loss=weighted_binary_crossentropy, \n",
      " 114:                   metrics = [dice_coef])\n",
      " 115:     \n",
      " 116:     \n",
      " 117:     model.fit(X_train, y_train, \n",
      " 118:                         validation_data = (X_valid, y_valid),\n",
      " 119:                           batch_size = 8,\n",
      " 120:                         epochs = 10, \n",
      " 121:                         verbose = 1,\n",
      " 122:                         shuffle = True\n",
      " 123:                         ) \n",
      " 124:                                   #validation_steps= 5,\n",
      " 125:                                   #callbacks = [tensorboard])\n",
      " 126: \n",
      " 127: \n",
      " 128:     score, dice = model.evaluate(X_valid, y_valid, verbose=0)\n",
      " 129:     print('Test accuracy:', dice)\n",
      " 130:     return {'loss': -dice, 'status': STATUS_OK, 'model': model}\n",
      " 131: \n",
      "\n",
      "Loading data...\n",
      "\n",
      "\n",
      "Loading data...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69 samples, validate on 29 samples\n",
      "Epoch 1/10\n",
      "69/69 [==============================] - 9s 124ms/step - loss: 0.3213 - dice_coef: 0.5789 - val_loss: 0.6729 - val_dice_coef: 0.1288\n",
      "Epoch 2/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2469 - dice_coef: 0.6550 - val_loss: 0.4878 - val_dice_coef: 0.1886\n",
      "Epoch 3/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2307 - dice_coef: 0.6564 - val_loss: 0.1798 - val_dice_coef: 0.5770\n",
      "Epoch 4/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2208 - dice_coef: 0.6609 - val_loss: 0.1900 - val_dice_coef: 0.6432\n",
      "Epoch 5/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2060 - dice_coef: 0.6653 - val_loss: 0.1712 - val_dice_coef: 0.5684\n",
      "Epoch 6/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1983 - dice_coef: 0.6655 - val_loss: 0.1872 - val_dice_coef: 0.6327\n",
      "Epoch 7/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1931 - dice_coef: 0.6584 - val_loss: 0.1998 - val_dice_coef: 0.5887\n",
      "Epoch 8/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1901 - dice_coef: 0.6633 - val_loss: 0.1700 - val_dice_coef: 0.5368\n",
      "Epoch 9/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1886 - dice_coef: 0.6515 - val_loss: 0.2425 - val_dice_coef: 0.2902\n",
      "Epoch 10/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1818 - dice_coef: 0.6538 - val_loss: 0.1673 - val_dice_coef: 0.5045\n",
      "Test accuracy: 0.5075241923332214\n",
      "Train on 69 samples, validate on 29 samples\n",
      "Epoch 1/10\n",
      "69/69 [==============================] - 6s 80ms/step - loss: 0.3262 - dice_coef: 0.5761 - val_loss: 0.3472 - val_dice_coef: 0.3116\n",
      "Epoch 2/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2437 - dice_coef: 0.6602 - val_loss: 0.3907 - val_dice_coef: 0.2718\n",
      "Epoch 3/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2310 - dice_coef: 0.6551 - val_loss: 0.1994 - val_dice_coef: 0.4117\n",
      "Epoch 4/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2157 - dice_coef: 0.6578 - val_loss: 0.1730 - val_dice_coef: 0.5549\n",
      "Epoch 5/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2078 - dice_coef: 0.6552 - val_loss: 0.1960 - val_dice_coef: 0.4448\n",
      "Epoch 6/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1976 - dice_coef: 0.6634 - val_loss: 0.2049 - val_dice_coef: 0.3706\n",
      "Epoch 7/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1920 - dice_coef: 0.6484 - val_loss: 0.1645 - val_dice_coef: 0.5774\n",
      "Epoch 8/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1853 - dice_coef: 0.6754 - val_loss: 0.1728 - val_dice_coef: 0.5011\n",
      "Epoch 9/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1803 - dice_coef: 0.6542 - val_loss: 0.1771 - val_dice_coef: 0.4508\n",
      "Epoch 10/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1797 - dice_coef: 0.6784 - val_loss: 0.1890 - val_dice_coef: 0.4946\n",
      "Test accuracy: 0.5001294016838074\n",
      "Train on 69 samples, validate on 29 samples\n",
      "Epoch 1/10\n",
      "69/69 [==============================] - 5s 79ms/step - loss: 0.3329 - dice_coef: 0.5760 - val_loss: 1.3019 - val_dice_coef: 0.0495\n",
      "Epoch 2/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2448 - dice_coef: 0.6477 - val_loss: 0.9056 - val_dice_coef: 0.0871\n",
      "Epoch 3/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2247 - dice_coef: 0.6584 - val_loss: 0.2485 - val_dice_coef: 0.6877\n",
      "Epoch 4/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2192 - dice_coef: 0.6508 - val_loss: 0.2176 - val_dice_coef: 0.4047\n",
      "Epoch 5/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2035 - dice_coef: 0.6618 - val_loss: 0.1661 - val_dice_coef: 0.5938\n",
      "Epoch 6/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1990 - dice_coef: 0.6563 - val_loss: 0.1809 - val_dice_coef: 0.4548\n",
      "Epoch 7/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1940 - dice_coef: 0.6448 - val_loss: 0.1822 - val_dice_coef: 0.5957\n",
      "Epoch 8/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1844 - dice_coef: 0.6715 - val_loss: 0.1863 - val_dice_coef: 0.5046\n",
      "Epoch 9/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1880 - dice_coef: 0.6390 - val_loss: 0.2905 - val_dice_coef: 0.5528\n",
      "Epoch 10/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1809 - dice_coef: 0.6710 - val_loss: 0.1721 - val_dice_coef: 0.6031\n",
      "Test accuracy: 0.6077147722244263\n",
      "Train on 69 samples, validate on 29 samples\n",
      "Epoch 1/10\n",
      "69/69 [==============================] - 6s 82ms/step - loss: 0.3343 - dice_coef: 0.5592 - val_loss: 1.1229 - val_dice_coef: 0.0404\n",
      "Epoch 2/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2454 - dice_coef: 0.6489 - val_loss: 0.2755 - val_dice_coef: 0.4179\n",
      "Epoch 3/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2287 - dice_coef: 0.6530 - val_loss: 0.2842 - val_dice_coef: 0.4628\n",
      "Epoch 4/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2225 - dice_coef: 0.6406 - val_loss: 0.4121 - val_dice_coef: 0.2979\n",
      "Epoch 5/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2095 - dice_coef: 0.6702 - val_loss: 0.3214 - val_dice_coef: 0.3218\n",
      "Epoch 6/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2020 - dice_coef: 0.6552 - val_loss: 0.1965 - val_dice_coef: 0.5564\n",
      "Epoch 7/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1938 - dice_coef: 0.6667 - val_loss: 0.1668 - val_dice_coef: 0.5589\n",
      "Epoch 8/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1875 - dice_coef: 0.6600 - val_loss: 0.1700 - val_dice_coef: 0.6069\n",
      "Epoch 9/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1835 - dice_coef: 0.6660 - val_loss: 0.1656 - val_dice_coef: 0.5485\n",
      "Epoch 10/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1795 - dice_coef: 0.6666 - val_loss: 0.1674 - val_dice_coef: 0.4807\n",
      "Test accuracy: 0.48317670822143555\n",
      "Train on 69 samples, validate on 29 samples\n",
      "Epoch 1/10\n",
      "69/69 [==============================] - 6s 84ms/step - loss: 0.3435 - dice_coef: 0.5482 - val_loss: 6.3194 - val_dice_coef: 0.5036\n",
      "Epoch 2/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2560 - dice_coef: 0.6384 - val_loss: 0.1877 - val_dice_coef: 0.5952\n",
      "Epoch 3/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2365 - dice_coef: 0.6448 - val_loss: 0.2019 - val_dice_coef: 0.6067\n",
      "Epoch 4/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2281 - dice_coef: 0.6557 - val_loss: 0.1775 - val_dice_coef: 0.6258\n",
      "Epoch 5/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2188 - dice_coef: 0.6483 - val_loss: 0.2156 - val_dice_coef: 0.4212\n",
      "Epoch 6/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2083 - dice_coef: 0.6609 - val_loss: 0.2531 - val_dice_coef: 0.6718\n",
      "Epoch 7/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1992 - dice_coef: 0.6573 - val_loss: 0.2117 - val_dice_coef: 0.6624\n",
      "Epoch 8/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1955 - dice_coef: 0.6596 - val_loss: 0.1797 - val_dice_coef: 0.5983\n",
      "Epoch 9/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1911 - dice_coef: 0.6553 - val_loss: 0.1679 - val_dice_coef: 0.5526\n",
      "Epoch 10/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1867 - dice_coef: 0.6578 - val_loss: 0.1925 - val_dice_coef: 0.6523\n",
      "Test accuracy: 0.6570910811424255\n",
      "\n",
      "Loading data...\n",
      "\n",
      "\n",
      "Loading data...\n",
      "\n",
      "Evalutation of best performing model:\n",
      "29/29 [==============================] - 0s 17ms/step\n",
      "[0.19246280193328857, 0.6570916175842285]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Dropout_1': 0, 'Dropout': 2, 'Dropout_2': 1}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials(),\n",
    "                                          eval_space=True,\n",
    "                                          notebook_name='hyperparam_tuning')\n",
    "    X_train, y_train, X_valid, y_valid = data()\n",
    "    print(\"Evalutation of best performing model:\")\n",
    "    print(best_model.evaluate(X_valid, y_valid))\n",
    "    print(\"Best performing model chosen hyper-parameters:\")\n",
    "    print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dropout_1': 0, 'Dropout': 2, 'Dropout_2': 1}\n"
     ]
    }
   ],
   "source": [
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'space' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b96da772d564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meval_hyperopt_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreal_param_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_hyperopt_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'space' is not defined"
     ]
    }
   ],
   "source": [
    "from hyperas.utils import eval_hyperopt_space\n",
    "real_param_values = eval_hyperopt_space(space, best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "\n",
    "path_img_train = data_path + 'images_69.npy'\n",
    "path_mask_train = data_path + 'masks_69.npy'\n",
    "\n",
    "path_img_valid = data_path + 'images_valid_29.npy'\n",
    "path_mask_valid = data_path + 'masks_valid_29.npy'\n",
    "\n",
    "# PATH definition\n",
    "initial_weights_path = \"./models/initial_weights.hdf5\"\n",
    "final_weights_path = \"./models/output_weights_batch_norm.hdf5\"\n",
    "\n",
    "\n",
    "# image dimension (array)\n",
    "img_rows = 512\n",
    "img_cols = 512\n",
    "\n",
    "batch_size = 14\n",
    "\n",
    "\n",
    "\n",
    "fill_mode = 'reflect'\n",
    "rotation_range= 10\n",
    "horizontal_flip= True\n",
    "vertical_flip = True\n",
    "rescale = 0\n",
    "zoom_range= 0.1\n",
    "channel_shift_range = 0.1\n",
    "width_shift_range = 0.1\n",
    "height_shift_range = 0.1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nb_initial_epochs = 1500\n",
    "apply_augmentation = True\n",
    "nb_step_predictions = 20\n",
    "\n",
    "learning_rate = 1e-3\n",
    "decay_rate = learning_rate / nb_initial_epochs\n",
    "\n",
    "\n",
    "apply_edt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters(timestamp, params):\n",
    "    param_file = str(param_path + 'params_' + timestamp + '.txt')\n",
    "    txt_file = open(param_file,'w')\n",
    "    txt_file.write(str(params))\n",
    "    txt_file.close()\n",
    "    \n",
    "def get_params():\n",
    "    param_dict = {\n",
    "    'img_rows':img_rows,\n",
    "    'img_cols' : img_cols,\n",
    "    'batch_size' : batch_size,\n",
    "    'fill_mode' : fill_mode,\n",
    "    'rotation_range': rotation_range,\n",
    "    'horizontal_flip': horizontal_flip,\n",
    "    'vertical_flip' : vertical_flip,\n",
    "    'rescale' : rescale,\n",
    "    'zoom_range': zoom_range,\n",
    "    'channel_shift_range' : channel_shift_range,\n",
    "    'width_shift_range' : width_shift_range,\n",
    "    'height_shift_range' : height_shift_range,\n",
    "    'nb_initial_epochs' : nb_initial_epochs,\n",
    "    'apply_augmentation' : apply_augmentation,\n",
    "    'nb_step_predictions': nb_step_predictions,\n",
    "    'learning_rate' : learning_rate,\n",
    "    'decay_rate' : decay_rate,\n",
    "    'apply_edt' : apply_edt\n",
    "    }\n",
    "    return param_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apply_augmentation': True,\n",
       " 'apply_edt': False,\n",
       " 'batch_size': 14,\n",
       " 'channel_shift_range': 0.1,\n",
       " 'decay_rate': 6.666666666666667e-07,\n",
       " 'fill_mode': 'reflect',\n",
       " 'height_shift_range': 0.1,\n",
       " 'horizontal_flip': True,\n",
       " 'img_cols': 512,\n",
       " 'img_rows': 512,\n",
       " 'learning_rate': 0.001,\n",
       " 'nb_initial_epochs': 1500,\n",
       " 'nb_step_predictions': 20,\n",
       " 'rescale': 0,\n",
       " 'rotation_range': 10,\n",
       " 'vertical_flip': True,\n",
       " 'width_shift_range': 0.1,\n",
       " 'zoom_range': 0.1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_path = '/home/malub_local/'\n",
    "save_parameters(format(time.strftime('%m%d_%H%M_%S')), get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/malub_local/activelearningADS/ceal'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
