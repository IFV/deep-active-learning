{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#from __future__ import division\n",
    "\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gzip\n",
    "from scipy.misc import imsave\n",
    "import time\n",
    "import scipy.ndimage\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Dropout\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "from keras import losses\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    # image dimension (array)\n",
    "    img_rows = 512\n",
    "    img_cols = 512\n",
    "\n",
    "    batch_size = 8\n",
    "    \n",
    "    def preprocessor(input_img, img_rows, img_cols):\n",
    "        \"\"\"\n",
    "        Resize input images to constants sizes\n",
    "        :param input_img: numpy array of images\n",
    "        :return: numpy array of preprocessed images\n",
    "        \"\"\"\n",
    "        output_img = np.ndarray((input_img.shape[0], input_img.shape[1], img_rows, img_cols), dtype=np.uint8)\n",
    "\n",
    "        for i in range(input_img.shape[0]):\n",
    "            output_img[i, 0] = cv2.resize(input_img[i, 0], (img_cols, img_rows), interpolation=cv2.INTER_CUBIC)\n",
    "        return output_img\n",
    "\n",
    "\n",
    "    def load_data(path_img, path_mask, img_rows, img_cols):\n",
    "        \"\"\"\n",
    "        Load data from project path\n",
    "        :return: [X, y] numpy arrays containing the [training, validation, test] data and their respective masks.\n",
    "        \"\"\"\n",
    "        print(\"\\nLoading data...\\n\")\n",
    "        X = np.load(path_img)\n",
    "        y = np.load(path_mask)\n",
    "\n",
    "        X = preprocessor(X, img_rows, img_cols)\n",
    "        y = preprocessor(y, img_rows, img_cols)\n",
    "\n",
    "        X = X.astype('float32')\n",
    "\n",
    "        mean = np.mean(X)  # mean for data centering\n",
    "        std = np.std(X)  # std for data normalization\n",
    "\n",
    "        X -= mean\n",
    "        X /= std\n",
    "\n",
    "        y = y.astype('float32')\n",
    "        y /= 255.  # scale masks to [0, 1]\n",
    "        return X, y\n",
    "    \n",
    "    data_path = '/home/malub_local/data/'\n",
    "\n",
    "    path_img_train = data_path + 'images_69.npy'\n",
    "    path_mask_train = data_path + 'masks_69.npy'\n",
    "\n",
    "    path_img_valid = data_path + 'images_valid_29.npy'\n",
    "    path_mask_valid = data_path + 'masks_valid_29.npy'\n",
    "\n",
    "    path_img_test = data_path + 'images_test_5.npy'\n",
    "    path_mask_test= data_path + 'masks_test_5.npy'\n",
    "\n",
    "    \n",
    "    X_train, y_train = load_data(path_img_train, path_mask_train, img_rows, img_cols )\n",
    "    X_valid, y_valid = load_data(path_img_valid, path_mask_valid, img_rows, img_cols)\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = True\n",
    "learning_rate = 1e-3\n",
    "nb_initial_epochs = 10\n",
    "decay_rate = learning_rate / nb_initial_epochs\n",
    "\n",
    "\n",
    "def create_model(X_train, y_train, X_valid, y_valid):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "        \n",
    "    \"\"\"\n",
    "    K.set_image_dim_ordering('th') \n",
    "    dropout = True\n",
    "    learning_rate = 1e-3\n",
    "    nb_initial_epochs = 10\n",
    "    decay_rate = learning_rate / nb_initial_epochs\n",
    "    \n",
    "    #### loss and metrics #####\n",
    "    def dice_coef(y_true, y_pred, smooth = 1.):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "    \"\"\"def dice_coef(y_true, y_pred, smooth=1):\n",
    "\n",
    "        intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "        return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "            # Calculate the binary crossentropy\n",
    "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "            # Apply the weights\n",
    "        weight_vector = y_true * 0.30 + (1. - y_true) * 0.70\n",
    "        weighted_b_ce = weight_vector * b_ce\n",
    "\n",
    "            # Return the mean error\n",
    "        return K.mean(weighted_b_ce)\n",
    "\n",
    "    \n",
    "    inputs = Input((1, img_rows, img_cols))\n",
    "    #conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "    batch1 = BatchNormalization(axis=1)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(batch1)\n",
    "\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(conv2)\n",
    "    batch2 = BatchNormalization(axis=1)(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(batch2)\n",
    "\n",
    "   \n",
    "    conv3 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(conv3)\n",
    "    batch3 = BatchNormalization(axis=1)(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(batch3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(conv4)\n",
    "    batch4 = BatchNormalization(axis=1)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(batch4)\n",
    "    \n",
    "    if dropout:\n",
    "        pool4 = Dropout(0.5)(pool4)\n",
    "   \n",
    "    conv5 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(conv5)\n",
    "\n",
    "    if dropout:\n",
    "        conv5 = Dropout({{choice([0.3, 0.5, 0.7])}})(conv5)    \n",
    "\n",
    "    up6_interm = UpSampling2D(size=(2, 2))(conv5)\n",
    "    \n",
    "    up6 = concatenate([up6_interm, conv4], axis=1)\n",
    "\n",
    "    conv6 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(conv6)\n",
    "    batch6 = BatchNormalization(axis=1)(conv6)\n",
    "\n",
    "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=1)\n",
    "    \n",
    "    if dropout:\n",
    "        up7 = Dropout({{choice([0.3, 0.5])}})(up7)  \n",
    "        \n",
    "    conv7 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(conv7)\n",
    "    batch7 = BatchNormalization(axis=1)(conv7)\n",
    "\n",
    "    up8 = concatenate([UpSampling2D(size=(2, 2))(batch7), conv2], axis=1)\n",
    "\n",
    "    conv8 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(conv8)\n",
    "    batch8 = BatchNormalization(axis=1)(conv8)\n",
    "\n",
    "    up9 = concatenate([UpSampling2D(size=(2, 2))(batch8), conv1], axis=1)\n",
    "\n",
    "    conv9 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(conv9)\n",
    "    batch9 = BatchNormalization(axis=1)(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation=\"sigmoid\")(batch9)\n",
    "\n",
    "    model = Model(outputs=conv10, inputs=inputs)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr = learning_rate, decay=decay_rate), loss=weighted_binary_crossentropy, \n",
    "                  metrics = [dice_coef])\n",
    "    \n",
    "    \n",
    "    model.fit(X_train, y_train, \n",
    "                        validation_data = (X_valid, y_valid),\n",
    "                          batch_size = 8,\n",
    "                        epochs = 10, \n",
    "                        verbose = 1,\n",
    "                        shuffle = True\n",
    "                        ) \n",
    "                                  #validation_steps= 5,\n",
    "                                  #callbacks = [tensorboard])\n",
    "\n",
    "\n",
    "    score, dice = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "    print('Test accuracy:', dice)\n",
    "    return {'loss': -dice, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import cv2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import gzip\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from scipy.misc import imsave\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import scipy.ndimage\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import *\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.merge import concatenate\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import *\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import *\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import losses\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.image import ImageDataGenerator\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dropout': hp.choice('Dropout', [0.3, 0.5, 0.7]),\n",
      "        'Dropout_1': hp.choice('Dropout_1', [0.3, 0.5]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: # image dimension (array)\n",
      "   3: img_rows = 512\n",
      "   4: img_cols = 512\n",
      "   5: \n",
      "   6: batch_size = 8\n",
      "   7: \n",
      "   8: def preprocessor(input_img, img_rows, img_cols):\n",
      "   9:     \"\"\"\n",
      "  10:     Resize input images to constants sizes\n",
      "  11:     :param input_img: numpy array of images\n",
      "  12:     :return: numpy array of preprocessed images\n",
      "  13:     \"\"\"\n",
      "  14:     output_img = np.ndarray((input_img.shape[0], input_img.shape[1], img_rows, img_cols), dtype=np.uint8)\n",
      "  15: \n",
      "  16:     for i in range(input_img.shape[0]):\n",
      "  17:         output_img[i, 0] = cv2.resize(input_img[i, 0], (img_cols, img_rows), interpolation=cv2.INTER_CUBIC)\n",
      "  18:     return output_img\n",
      "  19: \n",
      "  20: \n",
      "  21: def load_data(path_img, path_mask, img_rows, img_cols):\n",
      "  22:     \"\"\"\n",
      "  23:     Load data from project path\n",
      "  24:     :return: [X, y] numpy arrays containing the [training, validation, test] data and their respective masks.\n",
      "  25:     \"\"\"\n",
      "  26:     print(\"\\nLoading data...\\n\")\n",
      "  27:     X = np.load(path_img)\n",
      "  28:     y = np.load(path_mask)\n",
      "  29: \n",
      "  30:     X = preprocessor(X, img_rows, img_cols)\n",
      "  31:     y = preprocessor(y, img_rows, img_cols)\n",
      "  32: \n",
      "  33:     X = X.astype('float32')\n",
      "  34: \n",
      "  35:     mean = np.mean(X)  # mean for data centering\n",
      "  36:     std = np.std(X)  # std for data normalization\n",
      "  37: \n",
      "  38:     X -= mean\n",
      "  39:     X /= std\n",
      "  40: \n",
      "  41:     y = y.astype('float32')\n",
      "  42:     y /= 255.  # scale masks to [0, 1]\n",
      "  43:     return X, y\n",
      "  44: \n",
      "  45: data_path = '/home/malub_local/data/'\n",
      "  46: \n",
      "  47: path_img_train = data_path + 'images_69.npy'\n",
      "  48: path_mask_train = data_path + 'masks_69.npy'\n",
      "  49: \n",
      "  50: path_img_valid = data_path + 'images_valid_29.npy'\n",
      "  51: path_mask_valid = data_path + 'masks_valid_29.npy'\n",
      "  52: \n",
      "  53: path_img_test = data_path + 'images_test_5.npy'\n",
      "  54: path_mask_test= data_path + 'masks_test_5.npy'\n",
      "  55: \n",
      "  56: \n",
      "  57: X_train, y_train = load_data(path_img_train, path_mask_train, img_rows, img_cols )\n",
      "  58: X_valid, y_valid = load_data(path_img_valid, path_mask_valid, img_rows, img_cols)\n",
      "  59: \n",
      "  60: \n",
      "  61: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:         \n",
      "  13:     \"\"\"\n",
      "  14:     K.set_image_dim_ordering('th') \n",
      "  15:     dropout = True\n",
      "  16:     learning_rate = 1e-3\n",
      "  17:     nb_initial_epochs = 10\n",
      "  18:     decay_rate = learning_rate / nb_initial_epochs\n",
      "  19:     \n",
      "  20:     #### loss and metrics #####\n",
      "  21:     def dice_coef(y_true, y_pred, smooth = 1.):\n",
      "  22:         y_true_f = K.flatten(y_true)\n",
      "  23:         y_pred_f = K.flatten(y_pred)\n",
      "  24:         intersection = K.sum(y_true_f * y_pred_f)\n",
      "  25:         return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
      "  26: \n",
      "  27: \n",
      "  28:     \"\"\"def dice_coef(y_true, y_pred, smooth=1):\n",
      "  29: \n",
      "  30:         intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
      "  31:         return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\"\"\"\n",
      "  32: \n",
      "  33: \n",
      "  34: \n",
      "  35:     def weighted_binary_crossentropy(y_true, y_pred):\n",
      "  36:             # Calculate the binary crossentropy\n",
      "  37:         b_ce = K.binary_crossentropy(y_true, y_pred)\n",
      "  38: \n",
      "  39:             # Apply the weights\n",
      "  40:         weight_vector = y_true * 0.30 + (1. - y_true) * 0.70\n",
      "  41:         weighted_b_ce = weight_vector * b_ce\n",
      "  42: \n",
      "  43:             # Return the mean error\n",
      "  44:         return K.mean(weighted_b_ce)\n",
      "  45: \n",
      "  46:     \n",
      "  47:     inputs = Input((1, img_rows, img_cols))\n",
      "  48:     #conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
      "  49:     conv1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n",
      "  50:     conv1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(conv1)\n",
      "  51:     batch1 = BatchNormalization(axis=1)(conv1)\n",
      "  52:     pool1 = MaxPooling2D(pool_size=(2, 2))(batch1)\n",
      "  53: \n",
      "  54: \n",
      "  55:     conv2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(pool1)\n",
      "  56:     conv2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(conv2)\n",
      "  57:     batch2 = BatchNormalization(axis=1)(conv2)\n",
      "  58:     pool2 = MaxPooling2D(pool_size=(2, 2))(batch2)\n",
      "  59: \n",
      "  60:    \n",
      "  61:     conv3 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(pool2)\n",
      "  62:     conv3 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(conv3)\n",
      "  63:     batch3 = BatchNormalization(axis=1)(conv3)\n",
      "  64:     pool3 = MaxPooling2D(pool_size=(2, 2))(batch3)\n",
      "  65: \n",
      "  66:     conv4 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(pool3)\n",
      "  67:     conv4 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(conv4)\n",
      "  68:     batch4 = BatchNormalization(axis=1)(conv4)\n",
      "  69:     pool4 = MaxPooling2D(pool_size=(2, 2))(batch4)\n",
      "  70:     \n",
      "  71:     if dropout:\n",
      "  72:         pool4 = Dropout(0.5)(pool4)\n",
      "  73:    \n",
      "  74:     conv5 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(pool4)\n",
      "  75:     conv5 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(conv5)\n",
      "  76: \n",
      "  77:     if dropout:\n",
      "  78:         conv5 = Dropout(space['Dropout'])(conv5)    \n",
      "  79: \n",
      "  80:     up6_interm = UpSampling2D(size=(2, 2))(conv5)\n",
      "  81:     \n",
      "  82:     up6 = concatenate([up6_interm, conv4], axis=1)\n",
      "  83: \n",
      "  84:     conv6 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(up6)\n",
      "  85:     conv6 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(conv6)\n",
      "  86:     batch6 = BatchNormalization(axis=1)(conv6)\n",
      "  87: \n",
      "  88:     up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=1)\n",
      "  89:     \n",
      "  90:     if dropout:\n",
      "  91:         up7 = Dropout(space['Dropout_1'])(up7)  \n",
      "  92:         \n",
      "  93:     conv7 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(up7)\n",
      "  94:     conv7 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(conv7)\n",
      "  95:     batch7 = BatchNormalization(axis=1)(conv7)\n",
      "  96: \n",
      "  97:     up8 = concatenate([UpSampling2D(size=(2, 2))(batch7), conv2], axis=1)\n",
      "  98: \n",
      "  99:     conv8 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(up8)\n",
      " 100:     conv8 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(conv8)\n",
      " 101:     batch8 = BatchNormalization(axis=1)(conv8)\n",
      " 102: \n",
      " 103:     up9 = concatenate([UpSampling2D(size=(2, 2))(batch8), conv1], axis=1)\n",
      " 104: \n",
      " 105:     conv9 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(up9)\n",
      " 106:     conv9 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(conv9)\n",
      " 107:     batch9 = BatchNormalization(axis=1)(conv9)\n",
      " 108: \n",
      " 109:     conv10 = Conv2D(1, (1, 1), activation=\"sigmoid\")(batch9)\n",
      " 110: \n",
      " 111:     model = Model(outputs=conv10, inputs=inputs)\n",
      " 112: \n",
      " 113:     model.compile(optimizer=Adam(lr = learning_rate, decay=decay_rate), loss=weighted_binary_crossentropy, \n",
      " 114:                   metrics = [dice_coef])\n",
      " 115:     \n",
      " 116:     \n",
      " 117:     model.fit(X_train, y_train, \n",
      " 118:                         validation_data = (X_valid, y_valid),\n",
      " 119:                           batch_size = 8,\n",
      " 120:                         epochs = 10, \n",
      " 121:                         verbose = 1,\n",
      " 122:                         shuffle = True\n",
      " 123:                         ) \n",
      " 124:                                   #validation_steps= 5,\n",
      " 125:                                   #callbacks = [tensorboard])\n",
      " 126: \n",
      " 127: \n",
      " 128:     score, dice = model.evaluate(X_valid, y_valid, verbose=0)\n",
      " 129:     print('Test accuracy:', dice)\n",
      " 130:     return {'loss': -dice, 'status': STATUS_OK, 'model': model}\n",
      " 131: \n",
      "\n",
      "Loading data...\n",
      "\n",
      "\n",
      "Loading data...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69 samples, validate on 29 samples\n",
      "Epoch 1/10\n",
      "69/69 [==============================] - 9s 129ms/step - loss: 0.3359 - dice_coef: 0.5639 - val_loss: 1.4112 - val_dice_coef: 0.0228\n",
      "Epoch 2/10\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.2438 - dice_coef: 0.6503 - val_loss: 0.3842 - val_dice_coef: 0.3564\n",
      "Epoch 3/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2285 - dice_coef: 0.6578 - val_loss: 0.2146 - val_dice_coef: 0.4795\n",
      "Epoch 4/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2138 - dice_coef: 0.6545 - val_loss: 0.2489 - val_dice_coef: 0.3677\n",
      "Epoch 5/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2048 - dice_coef: 0.6625 - val_loss: 0.1966 - val_dice_coef: 0.4255\n",
      "Epoch 6/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1969 - dice_coef: 0.6588 - val_loss: 0.2068 - val_dice_coef: 0.3567\n",
      "Epoch 7/10\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.1948 - dice_coef: 0.6560 - val_loss: 0.2135 - val_dice_coef: 0.3645\n",
      "Epoch 8/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1934 - dice_coef: 0.6591 - val_loss: 0.1690 - val_dice_coef: 0.5325\n",
      "Epoch 9/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1876 - dice_coef: 0.6462 - val_loss: 0.2979 - val_dice_coef: 0.1603\n",
      "Epoch 10/10\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.1826 - dice_coef: 0.6630 - val_loss: 0.2063 - val_dice_coef: 0.4298\n",
      "Test accuracy: 0.4342750608921051\n",
      "Train on 69 samples, validate on 29 samples\n",
      "Epoch 1/10\n",
      "69/69 [==============================] - 6s 90ms/step - loss: 0.3295 - dice_coef: 0.5629 - val_loss: 1.1833 - val_dice_coef: 0.0559\n",
      "Epoch 2/10\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.2480 - dice_coef: 0.6520 - val_loss: 0.3173 - val_dice_coef: 0.5755\n",
      "Epoch 3/10\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.2325 - dice_coef: 0.6506 - val_loss: 0.6191 - val_dice_coef: 0.1507\n",
      "Epoch 4/10\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.2141 - dice_coef: 0.6724 - val_loss: 0.1719 - val_dice_coef: 0.5559\n",
      "Epoch 5/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2066 - dice_coef: 0.6620 - val_loss: 0.2391 - val_dice_coef: 0.6831\n",
      "Epoch 6/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2022 - dice_coef: 0.6639 - val_loss: 0.2231 - val_dice_coef: 0.3295\n",
      "Epoch 7/10\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.1939 - dice_coef: 0.6705 - val_loss: 0.1765 - val_dice_coef: 0.6228\n",
      "Epoch 8/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1978 - dice_coef: 0.6505 - val_loss: 0.1789 - val_dice_coef: 0.6752\n",
      "Epoch 9/10\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.1844 - dice_coef: 0.6802 - val_loss: 0.2076 - val_dice_coef: 0.6782\n",
      "Epoch 10/10\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.1778 - dice_coef: 0.6609 - val_loss: 0.2162 - val_dice_coef: 0.6944\n",
      "Test accuracy: 0.6980528831481934\n",
      "Train on 69 samples, validate on 29 samples\n",
      "Epoch 1/10\n",
      "69/69 [==============================] - 6s 92ms/step - loss: 0.3346 - dice_coef: 0.5596 - val_loss: 5.3969 - val_dice_coef: 0.5195\n",
      "Epoch 2/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2423 - dice_coef: 0.6504 - val_loss: 0.6091 - val_dice_coef: 0.5339\n",
      "Epoch 3/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2286 - dice_coef: 0.6644 - val_loss: 0.3052 - val_dice_coef: 0.6096\n",
      "Epoch 4/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2151 - dice_coef: 0.6589 - val_loss: 0.1737 - val_dice_coef: 0.5954\n",
      "Epoch 5/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2042 - dice_coef: 0.6621 - val_loss: 0.2385 - val_dice_coef: 0.6713\n",
      "Epoch 6/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1975 - dice_coef: 0.6501 - val_loss: 0.1787 - val_dice_coef: 0.4927\n",
      "Epoch 7/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1958 - dice_coef: 0.6731 - val_loss: 0.1674 - val_dice_coef: 0.5487\n",
      "Epoch 8/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1823 - dice_coef: 0.6651 - val_loss: 0.1882 - val_dice_coef: 0.6697\n",
      "Epoch 9/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1824 - dice_coef: 0.6653 - val_loss: 0.1622 - val_dice_coef: 0.6047\n",
      "Epoch 10/10\n",
      "69/69 [==============================] - 4s 61ms/step - loss: 0.1781 - dice_coef: 0.6672 - val_loss: 0.1726 - val_dice_coef: 0.5076\n",
      "Test accuracy: 0.5119447708129883\n",
      "Train on 69 samples, validate on 29 samples\n",
      "Epoch 1/10\n",
      "69/69 [==============================] - 7s 97ms/step - loss: 0.3341 - dice_coef: 0.5778 - val_loss: 1.0947 - val_dice_coef: 0.0855\n",
      "Epoch 2/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2424 - dice_coef: 0.6452 - val_loss: 0.5808 - val_dice_coef: 0.0595\n",
      "Epoch 3/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2211 - dice_coef: 0.6523 - val_loss: 0.5829 - val_dice_coef: 0.2088\n",
      "Epoch 4/10\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.2110 - dice_coef: 0.6525 - val_loss: 0.4170 - val_dice_coef: 0.1256\n",
      "Epoch 5/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2061 - dice_coef: 0.6468 - val_loss: 0.4186 - val_dice_coef: 0.4288\n",
      "Epoch 6/10\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.1956 - dice_coef: 0.6578 - val_loss: 0.1766 - val_dice_coef: 0.5080\n",
      "Epoch 7/10\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.1908 - dice_coef: 0.6573 - val_loss: 0.1670 - val_dice_coef: 0.5315\n",
      "Epoch 8/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1877 - dice_coef: 0.6593 - val_loss: 0.1960 - val_dice_coef: 0.5152\n",
      "Epoch 9/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1817 - dice_coef: 0.6460 - val_loss: 0.1709 - val_dice_coef: 0.5997\n",
      "Epoch 10/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1820 - dice_coef: 0.6629 - val_loss: 0.1692 - val_dice_coef: 0.4731\n",
      "Test accuracy: 0.4758508503437042\n",
      "Train on 69 samples, validate on 29 samples\n",
      "Epoch 1/10\n",
      "69/69 [==============================] - 6s 93ms/step - loss: 0.3357 - dice_coef: 0.5791 - val_loss: 1.0873 - val_dice_coef: 0.0659\n",
      "Epoch 2/10\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.2537 - dice_coef: 0.6329 - val_loss: 0.5738 - val_dice_coef: 0.0691\n",
      "Epoch 3/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2351 - dice_coef: 0.6555 - val_loss: 0.3470 - val_dice_coef: 0.3138\n",
      "Epoch 4/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2227 - dice_coef: 0.6489 - val_loss: 0.1951 - val_dice_coef: 0.4677\n",
      "Epoch 5/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2094 - dice_coef: 0.6562 - val_loss: 0.2118 - val_dice_coef: 0.4821\n",
      "Epoch 6/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1985 - dice_coef: 0.6598 - val_loss: 0.2164 - val_dice_coef: 0.4280\n",
      "Epoch 7/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1892 - dice_coef: 0.6649 - val_loss: 0.1766 - val_dice_coef: 0.5043\n",
      "Epoch 8/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1878 - dice_coef: 0.6623 - val_loss: 0.2194 - val_dice_coef: 0.3278\n",
      "Epoch 9/10\n",
      "69/69 [==============================] - 4s 61ms/step - loss: 0.1812 - dice_coef: 0.6544 - val_loss: 0.2449 - val_dice_coef: 0.2307\n",
      "Epoch 10/10\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.1749 - dice_coef: 0.6728 - val_loss: 0.1695 - val_dice_coef: 0.5387\n",
      "Test accuracy: 0.5417020320892334\n",
      "\n",
      "Loading data...\n",
      "\n",
      "\n",
      "Loading data...\n",
      "\n",
      "Evalutation of best performing model:\n",
      "29/29 [==============================] - 0s 17ms/step\n",
      "[0.21615144610404968, 0.6980530619621277]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Dropout_1': 0, 'Dropout': 2}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='hyperparam_tuning')\n",
    "    X_train, y_train, X_valid, y_valid = data()\n",
    "    print(\"Evalutation of best performing model:\")\n",
    "    print(best_model.evaluate(X_valid, y_valid))\n",
    "    print(\"Best performing model chosen hyper-parameters:\")\n",
    "    print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
