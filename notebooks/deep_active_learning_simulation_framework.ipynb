{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Active Learning simulation framework for Image Segmentation\n",
    "\n",
    "---\n",
    "**This notebook provides useful functions to implement active learning features for image segmentation.**<br>\n",
    "**It is applied in this case for histology segmentation of myelin sheath along the spinal cord.**\n",
    "\n",
    "Active Learning helps to efficiently train deep neural networks by selecting wisely the sample to annotate. \n",
    "\n",
    "---\n",
    "\n",
    "### Preliminary remarks\n",
    "##### Datasets\n",
    "* This code takes as input patches of images and their corresponding masks, preprocessed to specific shapes. <br>\n",
    "* It performs myelin segmentation (binary segmentation) on such images<br>\n",
    "* Raw data preprocessing is detailled in the **Dataset_preparation_v2.0.ipynb** notebook. <br>\n",
    "* A toy dataset composed of 2 SEM histology images preprocessed into 18 patches is available at **./deep_active_learning/datasets/** to run the notebook. \n",
    "\n",
    "\n",
    "##### Objectives\n",
    "* The objective of the notebook is to **SIMULATE** active learning on histology data to efficiently train models to do segmentation. \n",
    "* This active learning simulation framework is based on convolutional neural network to perform binary segmentation and MC-dropout to measure the uncertainty on the samples. \n",
    "\n",
    "\n",
    "##### GPUs\n",
    "* It is advised to run this notebook on GPUs since the models are trained multiple times (active learning iteration).\n",
    "* **Training time**: the results where obtain by realizing 10 experiments, of 15 active-learning iteration each (ie. adding up to 15 patches to the initial dataset). On 2 NVIDIA GPUs, 1 active learning iteration was taking about 20 minutes to run.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is organized in several parts as listed here: \n",
    "\n",
    "**1. Data Loading**<br>\n",
    "**2. Image segmentation**<br>\n",
    "    - Utils\n",
    "    - U-Net\n",
    "    - Score measures\n",
    "**3. Uncertainty measure**<br>\n",
    "**4. Active Learning Loop**<br>\n",
    "**5. Configuration parameters**<br>\n",
    "**6. Main - Training**<br>\n",
    "**7. Results analysis**<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gzip\n",
    "import time\n",
    "import random\n",
    "\n",
    "from keras import backend as K\n",
    "# from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Dropout\n",
    "from keras.layers import *\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, Callback\n",
    "from keras import losses\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import scipy.ndimage\n",
    "from scipy.misc import imsave\n",
    "from scipy.ndimage import morphology\n",
    "from numpy import linalg\n",
    "\n",
    "from itertools import islice\n",
    "from scipy.ndimage.morphology import distance_transform_edt as edt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run  this line if you are using a GPU\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3, 4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mel/Documents/neuropoly/deep_active_learning\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/mel/Documents/neuropoly/deep_active_learning'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(input_img, img_rows, img_cols):\n",
    "    \"\"\"\n",
    "    Resize input images to constants sizes\n",
    "    :param input_img: numpy array of images\n",
    "    :return: numpy array of preprocessed images\n",
    "    \"\"\"\n",
    "    output_img = np.ndarray((input_img.shape[0], input_img.shape[1], img_rows, img_cols), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(input_img.shape[0]):\n",
    "        output_img[i, 0] = cv2.resize(input_img[i, 0], (img_cols, img_rows), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    return output_img\n",
    "\n",
    "def load_data(path_img, path_mask, img_rows, img_cols):\n",
    "    \"\"\"\n",
    "    Load data from project path\n",
    "    :return: [X, y] numpy arrays containing the [training, validation, test] data and their respective masks.\n",
    "    \"\"\"\n",
    "    print(\"\\nLoading data...\\n\")\n",
    "    X = np.load(path_img)\n",
    "    y = np.load(path_mask)\n",
    "\n",
    "    X = preprocessor(X, img_rows, img_cols)\n",
    "    y = preprocessor(y, img_rows, img_cols)\n",
    "\n",
    "    X = X.astype('float32')\n",
    "\n",
    "    mean = np.mean(X)  # mean for data centering\n",
    "    std = np.std(X)  # std for data normalization\n",
    "\n",
    "    X -= mean\n",
    "    X /= std\n",
    "\n",
    "    y = y.astype('float32')\n",
    "    y /= 255.  # scale masks to [0, 1]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### loss and metrics #####\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"Compute Dice Coefficient between prediction and Ground Truth\n",
    "    :param y_true: ground truth\n",
    "    :param y_pred: prediction\n",
    "    :param smooth: avoid division by 0\n",
    "    :return: dice coefficient\n",
    "    \"\"\"\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    \"\"\"Dice loss to train a network\n",
    "    :param y_true: ground truth\n",
    "    :param y_pred: prediction\n",
    "    :return: dice loss function\n",
    "    \"\"\"\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    \"\"\"Compute the weighted binary cross-entropy\n",
    "    :param y_true: ground truth\n",
    "    :param y_pred: prediction\n",
    "    :return: weighted binary cross-entropy\n",
    "    \"\"\"\n",
    "    b_ce = K.binary_crossentropy(y_true, y_pred)  # Calculate the binary crossentropy\n",
    "    weight_vector = y_true * 0.30 + (1. - y_true) * 0.70  # Apply the weights\n",
    "    weighted_b_ce = weight_vector * b_ce        \n",
    "    return K.mean(weighted_b_ce) # Return the mean error\n",
    "\n",
    "\n",
    "##### data augmentation ######\n",
    "def data_generator():\n",
    "    \"\"\"\n",
    "    :return: Keras data generator. Data augmentation parameters.\n",
    "    \"\"\"\n",
    "    return ImageDataGenerator(\n",
    "        fill_mode = fill_mode,\n",
    "        rotation_range = rotation_range,\n",
    "        horizontal_flip = horizontal_flip,\n",
    "        vertical_flip = vertical_flip,\n",
    "        rescale = rescale,\n",
    "        zoom_range = zoom_range,\n",
    "        channel_shift_range = channel_shift_range,\n",
    "        width_shift_range = width_shift_range,\n",
    "        height_shift_range = height_shift_range)\n",
    "\n",
    "\n",
    "##### Monte-Carlo Dropout : keep dropout active at test time #####\n",
    "def call(self, inputs, training=None):\n",
    "    \"\"\"Override Dropout. Make it able at test time\n",
    "    \"\"\"\n",
    "    if 0. < self.rate < 1.:\n",
    "        noise_shape = self._get_noise_shape(inputs)\n",
    "        def dropped_inputs():\n",
    "            return K.dropout(inputs, self.rate, noise_shape,\n",
    "                             seed=self.seed)\n",
    "        if (training):\n",
    "            return K.in_train_phase(dropped_inputs, inputs, training=training)\n",
    "        else:\n",
    "            return K.in_test_phase(dropped_inputs, inputs, training=None)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def reset_weights(model):\n",
    "    \"\"\"Initialize weights of Neural Networks\n",
    "    \"\"\"\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run(session=session)\n",
    "            \n",
    "            \n",
    "def predict(data, model):\n",
    "    \"\"\"\n",
    "    Data prediction for a given model\n",
    "    :param data: input data to predict.\n",
    "    :param model: unet model.\n",
    "    :return: predictions.\n",
    "    \"\"\"\n",
    "    return model.predict(data, verbose=0)\n",
    "\n",
    "\n",
    "def save_parameters(timestamp, params):\n",
    "    \"\"\"Save current parameters to text file\n",
    "    :param timestamp:timestamp of the current run session\n",
    "    :param params: dictionary of current parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(param_path):\n",
    "        os.makedirs(param_path)\n",
    "        print(\"Directory \" , param_path ,  \" Created \")\n",
    "    \n",
    "    param_file = str(param_path + 'params_' + timestamp + '.txt')\n",
    "    txt_file = open(param_file,'w')\n",
    "    txt_file.write(str(params))\n",
    "    txt_file.close()\n",
    "    \n",
    "    \n",
    "def get_params():\n",
    "    \"\"\"Convert list of current parameters to dictionary \n",
    "    \"\"\"\n",
    "    param_dict = {\n",
    "    'img_rows':img_rows,\n",
    "    'img_cols' : img_cols,\n",
    "    'batch_size' : batch_size,\n",
    "    'fill_mode' : fill_mode,\n",
    "    'rotation_range': rotation_range,\n",
    "    'horizontal_flip': horizontal_flip,\n",
    "    'vertical_flip' : vertical_flip,\n",
    "    'rescale' : rescale,\n",
    "    'zoom_range': zoom_range,\n",
    "    'channel_shift_range' : channel_shift_range,\n",
    "    'width_shift_range' : width_shift_range,\n",
    "    'height_shift_range' : height_shift_range,\n",
    "    'nb_initial_epochs' : nb_initial_epochs,\n",
    "    'apply_augmentation' : apply_augmentation,\n",
    "    'nb_step_predictions': nb_step_predictions,\n",
    "    'steps_per_epoch' : steps_per_epoch,\n",
    "    'learning_rate' : learning_rate,\n",
    "    'decay_rate' : decay_rate,\n",
    "    'apply_edt' : apply_edt\n",
    "    }\n",
    "    return param_dict\n",
    "\n",
    "\n",
    "def save_history(history, timestamp, iteration, history_path):\n",
    "    \"\"\" Save training history in text file\n",
    "    :param history: training history (numpy array)\n",
    "    :param timestamp: timestamp of current run session\n",
    "    :param iteration: active learning iteration number\n",
    "    :param history_path: path to history text file\n",
    "    \"\"\"\n",
    "    history_file = str(history_path + timestamp + '_history_iter_' +  str(iteration) +'_' + '.txt')\n",
    "    txt_file = open(history_file,'w')\n",
    "    txt_file.write(str(history))\n",
    "    txt_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_full_bn(dropout):\n",
    "    inputs = Input((1, img_rows, img_cols))\n",
    "    conv1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "    batch1 = BatchNormalization(axis=1)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(batch1)\n",
    "    pool1 = Dropout(dropout_proba)(pool1)\n",
    "\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(conv2)\n",
    "    batch2 = BatchNormalization(axis=1)(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(batch2)\n",
    "    pool2 = Dropout(dropout_proba)(pool2)\n",
    "\n",
    "   \n",
    "    conv3 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(conv3)\n",
    "    batch3 = BatchNormalization(axis=1)(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(batch3)\n",
    "    pool3 = Dropout(dropout_proba)(pool3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(conv4)\n",
    "    batch4 = BatchNormalization(axis=1)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(batch4)\n",
    "    \n",
    "    if dropout:\n",
    "        pool4 = Dropout(dropout_proba)(pool4)\n",
    "   \n",
    "    conv5 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(conv5)\n",
    "\n",
    "    if dropout:\n",
    "        conv5 = Dropout(dropout_proba)(conv5)    \n",
    "\n",
    "    up6_interm = UpSampling2D(size=(2, 2))(conv5)\n",
    "    \n",
    "    up6 = concatenate([up6_interm, conv4], axis=1)\n",
    "\n",
    "    conv6 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(conv6)\n",
    "    batch6 = BatchNormalization(axis=1)(conv6)\n",
    "\n",
    "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=1)\n",
    "    \n",
    "    if dropout:\n",
    "        up7 = Dropout(dropout_proba)(up7)  \n",
    "        \n",
    "    conv7 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(conv7)\n",
    "    batch7 = BatchNormalization(axis=1)(conv7)\n",
    "\n",
    "    up8 = concatenate([UpSampling2D(size=(2, 2))(batch7), conv2], axis=1)\n",
    "    up8 = Dropout(dropout_proba)(up8)\n",
    "\n",
    "    conv8 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(conv8)\n",
    "    batch8 = BatchNormalization(axis=1)(conv8)\n",
    "\n",
    "    up9 = concatenate([UpSampling2D(size=(2, 2))(batch8), conv1], axis=1)\n",
    "    up9 = Dropout(dropout_proba)(up9)\n",
    "\n",
    "    conv9 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(conv9)\n",
    "    batch9 = BatchNormalization(axis=1)(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation=\"sigmoid\")(batch9)\n",
    "\n",
    "    model = Model(outputs=conv10, inputs=inputs)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr = learning_rate, decay=decay_rate), loss= dice_coef_loss, \n",
    "                  metrics = [dice_coef])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Score Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Score measure\n",
    "\n",
    "def numeric_score(y_pred, y_true):\n",
    "    \"\"\"Compute True Positive, True Negative, False Positive, False Negative classifications\n",
    "    between a prediction and its ground truth\n",
    "    :param y_pred: prediction\n",
    "    :param y_true: ground truth\n",
    "    :return: True Positive, True Negative, False Positive, False Negative\n",
    "    \"\"\"\n",
    "    y_pred = y_pred.astype(int)\n",
    "    y_true = y_true.astype(int)\n",
    "    FP = float(np.sum((y_pred == 1) & (y_true == 0)))\n",
    "    FN = float(np.sum((y_pred == 0) & (y_true == 1)))\n",
    "    TP = float(np.sum((y_pred == 1) & (y_true == 1)))\n",
    "    TN = float(np.sum((y_pred == 0) & (y_true == 0)))\n",
    "    return FP, FN, TP, TN\n",
    "\n",
    "\n",
    "def jaccard_score(y_pred, y_true):\n",
    "    \"\"\"Compute Jaccard Score (= Intersection / Union) between a prediction and its ground truth\n",
    "    :param y_pred: prediction\n",
    "    :param y_true: ground truth\n",
    "    :return: Jaccard score value\n",
    "    \"\"\"\n",
    "    intersection = (y_pred * y_true).sum()\n",
    "    union = y_pred.sum() + y_true.sum() - intersection\n",
    "    if union == 0:\n",
    "        return 1.\n",
    "    else:\n",
    "        return float(intersection)/union\n",
    "    \n",
    "    \n",
    "def pixel_wise_accuracy(y_true, y_pred):\n",
    "    \"\"\"Compute Pixel-wise accuracy (= number of well classified pixel / total number of pixel) \n",
    "    between a prediction and its ground truth\n",
    "    :param y_pred: prediction\n",
    "    :param y_true: ground truth\n",
    "    :return: Pixel-wise accuracy value\n",
    "    \"\"\"\n",
    "    y_true_f = y_true.reshape([1, img_rows * img_cols])\n",
    "    y_pred_f = y_pred.reshape([1, img_rows * img_cols])\n",
    "    return 1 - np.count_nonzero(y_pred_f - y_true_f) / y_true_f.shape[1]\n",
    "    \n",
    "\n",
    "def precision_score(y_pred, y_true):\n",
    "    \"\"\"Compute precision (= TP / (TP+FP)) between a prediction and its ground truth\n",
    "    :param y_pred: prediction\n",
    "    :param y_true: ground truth\n",
    "    :return: Precision score value\n",
    "    \"\"\"\n",
    "    FP, FN, TP, TN = numeric_score(y_pred, y_true)\n",
    "    if (TP + FP) <= 0:\n",
    "        return 0.\n",
    "    else:\n",
    "        return np.divide(TP, TP + FP) \n",
    "    \n",
    "\n",
    "def sensitivity_score(y_pred, y_true):\n",
    "    \"\"\"Compute sensitivity (= TP / (TP+FN)) between a prediction and its ground truth\n",
    "    :param y_pred: prediction\n",
    "    :param y_true: ground truth\n",
    "    :return: Sensitivity score value\n",
    "    \"\"\"\n",
    "    FP, FN, TP, TN = numeric_score(y_pred, y_true)\n",
    "    if (TP + FN) <= 0:\n",
    "        return 0.\n",
    "    else:\n",
    "        return np.divide(TP, TP + FN) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Uncertainty Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_transform(sample):\n",
    "    \"\"\"\n",
    "    Range normalization for 255 range of values\n",
    "    :param sample: numpy array for normalize\n",
    "    :return: normalize numpy array\n",
    "    \"\"\"\n",
    "    if (np.max(sample) == 1):\n",
    "        sample = sample * 255\n",
    "\n",
    "    m = 255 / (np.max(sample) - np.min(sample))\n",
    "    n = 255 - m * np.max(sample)\n",
    "    return (m * sample + n) / 255\n",
    "\n",
    "\n",
    "def compute_dice_coef(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Dice-Coefficient of a prediction given its ground truth.\n",
    "    :param y_true: Ground truth.\n",
    "    :param y_pred: Prediction.\n",
    "    :return: Dice-Coefficient value.\n",
    "    \"\"\"\n",
    "    smooth = 1.  # smoothing value to deal zero denominators.\n",
    "    y_true_f = y_true.reshape([1, img_rows * img_cols])\n",
    "    y_pred_f = y_pred.reshape([1, img_rows * img_cols])\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n",
    "\n",
    "def compute_uncertain(sample, model):\n",
    "    \"\"\"\n",
    "    Computes uncertainty map for a given sample and its prediction for a given model, based on the\n",
    "    number of step predictions defined in constants file.\n",
    "    :param sample: input sample.\n",
    "    :param model: unet model with Dropout layers.\n",
    "    :return: averaged-thresholded predictions after nb_steps_prediction samples\n",
    "    :return: overall uncertainty (not map)\n",
    "    :return: uncertainty_map.\n",
    "    \"\"\"\n",
    "    X = np.zeros([1, img_rows, img_cols])\n",
    "\n",
    "    for t in range(nb_step_predictions):\n",
    "        prediction = model.predict(sample, verbose=0).reshape([1, img_rows, img_cols])\n",
    "        X = np.concatenate((X, prediction))\n",
    "\n",
    "    X = np.delete(X, [0], 0)\n",
    "    # averaged-thresholded predictions after np_step_prediction predicted samples\n",
    "    X_prediction = cv2.threshold(np.mean(X, axis=0), threshold, 1, cv2.THRESH_BINARY)[1].astype('uint8')\n",
    "\n",
    "    if (apply_edt):\n",
    "        # apply distance transform normalization.\n",
    "        var = np.var(X, axis=0)\n",
    "        transform = range_transform(edt(1 - prediction))\n",
    "        return X_prediction, np.sum(np.multiply(var, transform)),  np.multiply(var, transform)\n",
    "\n",
    "    else:\n",
    "        return X_prediction, np.sum(np.sqrt(np.var(X, axis=0))), np.var(X, axis=0)\n",
    "\n",
    "\n",
    "def compute_uncertain_map(sample, prediction, model):\n",
    "    \"\"\"\n",
    "    Computes uncertainty map for a given sample and its prediction for a given model, based on the\n",
    "    number of step predictions defined in constants file.\n",
    "    :param sample: input sample.\n",
    "    :param prediction: input sample prediction.\n",
    "    :param model: unet model with Dropout layers.\n",
    "    :return: uncertainty map.\n",
    "    \"\"\"\n",
    "    X = np.zeros([1, img_rows, img_cols])\n",
    "\n",
    "    for t in range(nb_step_predictions):\n",
    "        prediction = model.predict(sample, verbose=0).reshape([1, img_rows, img_cols])\n",
    "        X = np.concatenate((X, prediction))\n",
    "\n",
    "    X = np.delete(X, [0], 0)\n",
    "\n",
    "    if (apply_edt):\n",
    "        # apply distance transform normalization.\n",
    "        var = np.var(X, axis=0)\n",
    "        transform = range_transform(edt(prediction))\n",
    "        return var * transform\n",
    "\n",
    "    else:\n",
    "        return np.var(X, axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Active Learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_annotated_sample(original, added, added_index):\n",
    "    \"\"\"append the annotated samples to the training set\n",
    "    \"\"\"\n",
    "    return np.vstack((original, added[added_index] ))\n",
    "\n",
    "\n",
    "def data_splitting(X_set, training_size, validation_size, test_size):\n",
    "    \"\"\"generate indexes to split initial dataset in train, validation and test sets\n",
    "    :param X_set: initial dataset (all the samples)\n",
    "    :param training_size: size of the training set\n",
    "    :param validation_size: size of the validation set\n",
    "    :param test_size: size of the test set\n",
    "    :return: indexes for labelled, unlabelled, test and validation sets\n",
    "    \"\"\"\n",
    "    unlabelled_size = len(X_set) - training_size - validation_size - test_size\n",
    "    index_labelled = np.arange(0, training_size, 1)\n",
    "    index_unlabelled = np.arange(training_size, training_size + unlabelled_size, 1)\n",
    "    index_test = np.arange(training_size + unlabelled_size, training_size + unlabelled_size + test_size, 1)\n",
    "    index_validation = np.arange(training_size + unlabelled_size + test_size, len(X_set), 1)\n",
    "    return index_labelled, index_unlabelled, index_test, index_validation\n",
    "\n",
    "def split_for_simulation(X, y, index_labelled, index_unlabelled, index_test, index_validation):\n",
    "    \"\"\"split initial dataset in train, validation and test sets based on indexes \n",
    "    \"\"\"\n",
    "    X_labelled_0 = X[index_labelled]\n",
    "    y_labelled_0 = y[index_labelled]\n",
    "\n",
    "    X_unlabelled_0 = X[index_unlabelled]\n",
    "    y_unlabelled_0 = y[index_unlabelled]\n",
    "\n",
    "    X_test = X[index_test]\n",
    "    y_test = y[index_test]\n",
    "    \n",
    "    X_validation = X[index_validation]\n",
    "    y_validation = y[index_validation]\n",
    "    \n",
    "    return X_labelled_0, y_labelled_0, X_unlabelled_0, y_unlabelled_0, X_test, y_test, X_validation, y_validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(model, X_test, y_test, score_global):\n",
    "    \"\"\"Compute predictions scores on test set for the current active learning iteration\n",
    "    :param model: trained Unet model\n",
    "    :param X_test: test set\n",
    "    :param y_test: test ground truth\n",
    "    :param score global: numpy array with previous scores\n",
    "    :return: numpy array with scores\n",
    "    \"\"\"\n",
    "    uncertain_map = []\n",
    "    uncertain = np.zeros(len(X_test))\n",
    "    dice = np.zeros(len(X_test))\n",
    "    jaccard = np.zeros(len(X_test))\n",
    "    accuracy = np.zeros(len(X_test))\n",
    "    sensitivity = np.zeros(len(X_test))\n",
    "    precision = np.zeros(len(X_test))\n",
    "    \n",
    "    print(\"Computing predictions on test data ...\\n\")\n",
    "    predictions = predict(X_test, model)\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        sample = X_test[i].reshape([1, 1, img_rows, img_cols])\n",
    "        sample_prediction = cv2.threshold(predictions[i], threshold, 1, cv2.THRESH_BINARY)[1].astype('uint8')\n",
    "        dice[i] = compute_dice_coef(y_test[i][0], sample_prediction)\n",
    "        jaccard[i] = jaccard_score(sample_prediction, y_test[i][0, :, :])\n",
    "        accuracy[i] = pixel_wise_accuracy(sample_prediction, y_test[i][0, :, :])\n",
    "        sensitivity[i] = sensitivity_score(sample_prediction, y_test[i][0, :, :])\n",
    "        precision[i] = precision_score(sample_prediction, y_test[i][0, :, :])\n",
    "        _ , uncertain[i], uncertain_map_i = compute_uncertain(sample, model)\n",
    "        #uncertain_map_i = compute_uncertain_map(sample, sample_prediction, model)\n",
    "        uncertain_map.append(uncertain_map_i)\n",
    "\n",
    "    uncertain_map_array = np.asarray(uncertain_map)\n",
    "    print(\"Done computing predictions on test data\")\n",
    "    metrics_array = np.array([np.mean(dice), np.mean(jaccard), np.mean(accuracy), np.mean(precision),\n",
    "                                np.mean(sensitivity)])\n",
    "    score_global = np.vstack((score_global, metrics_array))\n",
    "    return score_global\n",
    "\n",
    "\n",
    "def balance_uncertainty(sample_pred, uncertainty, apply_edt):\n",
    "    \"\"\"Multiply uncertainty by a ratio to overcome unbalanced class issues\n",
    "    :param sample_pred: prediction for a sample\n",
    "    :param uncertainty: associated uncertainty measure\n",
    "    :return: scaled uncertainty measure\n",
    "    \"\"\"\n",
    "    if apply_edt == True:\n",
    "        ratio = 1\n",
    "    else:\n",
    "        ratio = (1.0 / ((np.count_nonzero(sample_pred) + 1.0) / (512*512* 1.0)))\n",
    "    return uncertainty * ratio\n",
    "\n",
    "\n",
    "def uncertainty_for_ranking(model, X_set):\n",
    "    \"\"\"Compute uncertainty and uncertainty map for a set of images\n",
    "    :param model: trained Unet model\n",
    "    :param X_set: set of data\n",
    "    :return: numpy array with uncertainty value and numpy array of uncertainty maps for each image of X_set,\n",
    "    and final prediction for X_set, which is an AVERAGE OF THE nb_step_prediction FORWARD PASS PREDICTIONS\n",
    "    \"\"\"\n",
    "    # uncertainty computation for unlabelled\n",
    "    print(\"Computing predictions for unlabelled data ...\\n\")\n",
    "    uncertain_map = []\n",
    "    uncertainty = np.zeros(len(X_set))\n",
    "    predictions = predict(X_set, model)\n",
    "    X = []\n",
    "    for i in range(len(X_set)):\n",
    "        sample = X_set[i].reshape([1, 1, img_rows, img_cols])\n",
    "        sample_prediction = cv2.threshold(predictions[i], threshold, 1, cv2.THRESH_BINARY)[1].astype('uint8')\n",
    "        X_i, uncertainty_i, uncertain_map_i = compute_uncertain(sample, model)\n",
    "        uncertainty[i] = balance_uncertainty(X_i, uncertainty_i, apply_edt) # scale uncertainty\n",
    "        uncertain_map.append(uncertain_map_i)\n",
    "        X.append(X_i)\n",
    "    uncertain_map_array = np.asarray(uncertain_map)\n",
    "    return uncertainty, uncertain_map, X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_be_added_index(active_learning_iter_size, index, uncertain_unlabelled, dont_random):\n",
    "    \"\"\" Select the most uncertain samples, retrieve its indice in order to add it to the training set\n",
    "    :param active_learning_iter_size: number of samples selected to be added to the training set after each AL iteration\n",
    "    :param index: list of already selected unlabelled samples\n",
    "    :param uncertain_unlabelled: Uncertainty measures for all unlabelled samples\n",
    "    :param dont_random: number of randomly selected samples to be added to the training set (< active_learning_iter_size)\n",
    "    :return: list of selected samples indices after each AL iteration\n",
    "    \"\"\"\n",
    "    uncert_unlab = np.copy(uncertain_unlabelled)\n",
    "    if dont_random == 0:\n",
    "        for m in range(active_learning_iter_size):\n",
    "            to_be_added = np.argmax(uncert_unlab)\n",
    "            while to_be_added in index:\n",
    "                uncert_unlab[to_be_added] = 0\n",
    "                to_be_added = np.argmax(uncert_unlab)\n",
    "            index.append(to_be_added)\n",
    "        return index\n",
    "    else:\n",
    "        for m in range(active_learning_iter_size - dont_random):\n",
    "            to_be_added = np.argmax(uncertain_unlabelled)\n",
    "            while to_be_added in index:\n",
    "                uncertain_unlabelled[to_be_added] = 0\n",
    "                to_be_added = np.argmax(uncertain_unlabelled)\n",
    "            index = np.append(np.asarray(index), np.asarray(to_be_added))\n",
    "        left_to_pick = np.setdiff1d(np.arange(len(uncertain_unlabelled)), np.asarray(index))\n",
    "        random_index = np.random.choice(left_to_pick, dont_random, replace=False)\n",
    "        return np.hstack((np.asarray(index, dtype = int), np.asarray(random_index, dtype = int)))\n",
    "\n",
    "\n",
    "def to_be_added_random(active_learning_iter_size, index, uncertain_unlabelled):\n",
    "    \"\"\" Select randomly samples to add to the training set (baseline)\n",
    "    :param active_learning_iter_size: number of samples selected to be added to the training set after each AL iteration\n",
    "    :param index: list of already selected unlabelled samples\n",
    "    :param uncertain_unlabelled: Uncertainty measures for all unlabelled samples\n",
    "    :return: list of selected samples indices after each AL iteration\n",
    "    \"\"\"\n",
    "    lim = 0\n",
    "    for m in range(active_learning_iter_size):\n",
    "        to_be_added = random.randint(0, len(uncertain_unlabelled) -1)\n",
    "        while to_be_added in index:\n",
    "            if lim > 100: \n",
    "                to_be_added = np.delete[np.arange(len(uncertain_unlabelled), index)[0]]\n",
    "            to_be_added = random.randint(0, len(uncertain_unlabelled)-1)\n",
    "            lim = lim + 1\n",
    "        index.append(to_be_added)\n",
    "    return index\n",
    "\n",
    "def save_results(result, name, session_name, random_bool):   \n",
    "    if random_bool == True:\n",
    "        np.save(str(results_path + name + session_name + \"_random\" ), result)\n",
    "    else:\n",
    "        np.save(str(results_path + name + session_name), result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Configuration\n",
    "**Default toy values for each parameters**<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH definition\n",
    "param_path = '../params/'\n",
    "results_path = './results/'\n",
    "history_path = '../history/'\n",
    "\n",
    "# paths to datasets\n",
    "path_img = \"./datasets/SEM_image.npy\"\n",
    "path_mask = \"./datasets/SEM_mask.npy\"\n",
    "\n",
    "\n",
    "# image dimension (array)\n",
    "img_rows = 512\n",
    "img_cols = 512\n",
    "\n",
    "# data augmentation\n",
    "fill_mode = 'reflect'\n",
    "rotation_range= 10\n",
    "horizontal_flip= True\n",
    "vertical_flip = True\n",
    "rescale = 0\n",
    "zoom_range= 0.2\n",
    "channel_shift_range = 0.1\n",
    "width_shift_range = 0.1\n",
    "height_shift_range = 0.1\n",
    "\n",
    "# Dropout at test time\n",
    "K.set_image_dim_ordering('th')  # Theano dimension ordering in this code\n",
    "smooth = 1.\n",
    "Dropout.call = call\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 5 \n",
    "nb_initial_epochs = 3 # number of epochs\n",
    "steps_per_epoch = 2 # number of steps per epochs\n",
    "apply_augmentation = True # True or False: Apply Data Augmentation\n",
    "nb_step_predictions = 5 # Number of Monte-Carlo samples to compute the uncertainty\n",
    "\n",
    "dropout_proba = 0.2 # dropout probability\n",
    "\n",
    "learning_rate = 1e-2\n",
    "decay_rate = learning_rate / nb_initial_epochs\n",
    "\n",
    "apply_edt = True # apply euclidean distance transform when computing the uncertainty\n",
    "threshold = 0.5 # prediction threshold\n",
    "\n",
    "nb_active_learning_iter = 3 # number of active learning iteration\n",
    "active_learning_iter_size = 1 # number of patch added during each active learning iteration\n",
    "\n",
    "\n",
    "# Datasets splitting\n",
    "init_labelled_size = 3 \n",
    "test_size = 3\n",
    "validation_size = 4\n",
    "\n",
    "random_bool = False # True: Baseline (random selection), False: Uncertainty based selection\n",
    "dont_random = 0 # Number of randomly picked samples among the selected sample (e.g. adding 2 patches: \n",
    "                #1 selected with uncertainty, 1 picked randomly)\n",
    "\n",
    "# name to save the results\n",
    "session_name = \"toy_trial\"\n",
    "\n",
    "# number of experiments: multiple experiments should be run to obtain a proper mean and \n",
    "# standard deviation of the results and overcome randomness of the network.\n",
    "nb_experiement = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "\n",
      "\n",
      "\n",
      "*****************EXPERIMENT 1 IS STARTING********************\n",
      "('X_labelled shape = ', (3, 1, 512, 512), 'X_unlabelled shape = ', (8, 1, 512, 512))\n",
      "('X_test shape = ', (3, 1, 512, 512), 'X_validation shape = ', (4, 1, 512, 512))\n",
      "------------DATA SPLITTING DONE---------\n",
      "\n",
      "TIMESTAMP: 0118_1705_23\n",
      "\n",
      "---------Starting AL Iteration number 0----------\n",
      "------------TRAINING -----------\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-66683de0913d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m                                         \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                                         callbacks = [tensorboard])\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0msave_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# save training history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mel/anaconda3/envs/dal_venv/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mel/anaconda3/envs/dal_venv/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mel/anaconda3/envs/dal_venv/lib/python2.7/site-packages/keras/engine/training_generator.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    236\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                             verbose=0)\n\u001b[0m\u001b[1;32m    239\u001b[0m                     \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                     \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mel/anaconda3/envs/dal_venv/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1109\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m                                          steps=steps)\n\u001b[0m\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m     def predict(self, x,\n",
      "\u001b[0;32m/Users/mel/anaconda3/envs/dal_venv/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mel/anaconda3/envs/dal_venv/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mel/anaconda3/envs/dal_venv/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mel/anaconda3/envs/dal_venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load data\n",
    "X_full, y_full = load_data(path_img, path_mask, img_rows, img_cols )\n",
    "\n",
    "index_labelled, index_unlabelled, index_test, index_validation = data_splitting(X_full, init_labelled_size,\n",
    "                                                                                validation_size, test_size)\n",
    "\n",
    "# split data into train (labelled, 30%), unlabelled (active learning simulation, 50%), test (20%)\n",
    "X_labelled_0, y_labelled_0, X_unlabelled_0, y_unlabelled_0, X_test, y_test , X_validation, y_validation = split_for_simulation(\n",
    "                                                                                                X_full, \n",
    "                                                                                                y_full,\n",
    "                                                                                                index_labelled,\n",
    "                                                                                                index_unlabelled,\n",
    "                                                                                                index_test, \n",
    "                                                                                                index_validation)\n",
    "\n",
    "score_average = []\n",
    "indexes_experiments = []\n",
    "\n",
    "for r in range(nb_experiement):\n",
    "    print(\"\\n\\n*****************EXPERIMENT \" + str(r+1) + \" IS STARTING********************\")\n",
    "    \n",
    "    rank_iter = np.zeros((nb_active_learning_iter, len(index_unlabelled)))\n",
    "    uncertain_iter = np.zeros((nb_active_learning_iter, len(index_unlabelled)))\n",
    "\n",
    "    # full experience\n",
    "    uncertainty_unlabelled = np.zeros((nb_active_learning_iter, len(index_unlabelled)))\n",
    "    to_be_annotated_index = []\n",
    "\n",
    "    print(\"X_labelled shape = \", X_labelled_0.shape, \"X_unlabelled shape = \", X_unlabelled_0.shape)\n",
    "    print(\"X_test shape = \", X_test.shape,\"X_validation shape = \", X_validation.shape )\n",
    "    print(\"------------DATA SPLITTING DONE---------\\n\")\n",
    "\n",
    "    timestamp = format(time.strftime('%m%d_%H%M_%S'))\n",
    "    print \"TIMESTAMP:\", timestamp\n",
    "\n",
    "\n",
    "    # START ACTIVE LEARNING ITERATION\n",
    "    for i in range(nb_active_learning_iter):\n",
    "        \n",
    "        if random_bool == True:\n",
    "            model_path = \"../models/AL_model_random_\" + str(i) + \".hdf5\"\n",
    "        else:\n",
    "            model_path = \"../models/AL_model_test_\" + str(i) + \".hdf5\"     # save trained model after each training\n",
    "        \n",
    "        print(\"\\n---------Starting AL Iteration number \" + str(i) + '----------')\n",
    "        \n",
    "        if i == 0: #initialization \n",
    "            X_labelled = X_labelled_0\n",
    "            y_labelled = y_labelled_0\n",
    "            X_unlabelled = X_unlabelled_0\n",
    "            y_unlabelled = y_unlabelled_0\n",
    "            score_global = np.zeros(5)\n",
    "        else:\n",
    "            # select the samples that are going to be annotated by expert   \n",
    "            uncertain_iter[i] = uncertain_unlabelled\n",
    "            if random_bool == True:\n",
    "                to_be_annotated_index = to_be_added_random(active_learning_iter_size, to_be_annotated_index, \n",
    "                                                           uncertain_unlabelled)\n",
    "            else:\n",
    "                to_be_annotated_index = to_be_added_index(active_learning_iter_size, to_be_annotated_index, \n",
    "                                                          uncertain_unlabelled, dont_random)\n",
    "                \n",
    "            \n",
    "            print \"List of samples to be annotated by oracle=\",  to_be_annotated_index\n",
    "            # add this samples + masks to the train set \n",
    "            X_labelled = add_annotated_sample(X_labelled_0, X_unlabelled_0, to_be_annotated_index)\n",
    "            y_labelled = add_annotated_sample(y_labelled_0, y_unlabelled_0, to_be_annotated_index)\n",
    "\n",
    "\n",
    "            print(\"new X_labelled shape = \", X_labelled.shape, \"new y_labelled shape =\", y_labelled.shape)\n",
    "            print(\"new X_unlabelled shape=\", X_unlabelled.shape, \"new y_unlabelled shape =\", y_unlabelled.shape)\n",
    "\n",
    "\n",
    "        ## Retrain with new dataset\n",
    "\n",
    "        # data augmentation\n",
    "        seed = 1\n",
    "        train_img_generator = data_generator().flow(X_labelled, seed = seed, batch_size = batch_size,  shuffle=False)\n",
    "        train_mask_generator = data_generator().flow(y_labelled, seed = seed, batch_size = batch_size, shuffle=False)\n",
    "        train_generator = zip(train_img_generator, train_mask_generator)\n",
    "\n",
    "\n",
    "        # load model\n",
    "        model = unet_full_bn(dropout = True)\n",
    "\n",
    "        # retrain from scratch after each iteration\n",
    "        reset_weights(model)\n",
    "\n",
    "        print(\"------------TRAINING -----------\")\n",
    "        save_parameters(timestamp, get_params()) # save parameters\n",
    "        tensorboard = TensorBoard(log_dir = str(\"../logs2/\" + str(timestamp)) + \"_iter_\" + str(i), write_images=True) # tensorboard\n",
    "        history = model.fit_generator(train_generator, \n",
    "                                        validation_data = (X_validation, y_validation), \n",
    "                                        epochs = nb_initial_epochs, \n",
    "                                        verbose = 2,\n",
    "                                        shuffle = True,\n",
    "                                        steps_per_epoch = steps_per_epoch, \n",
    "                                        callbacks = [tensorboard])\n",
    "\n",
    "        save_history(history.history, timestamp, i, history_path) # save training history\n",
    "        model.save(model_path) # save fully trained model\n",
    "        \n",
    "        save_results(uncertain_iter, \"uncertainty_iter_\", session_name, random_bool)\n",
    "\n",
    "        print(\"----------- Training done --------------\")\n",
    "\n",
    "        print(\"\\n----------- RANKING -----------\")\n",
    "        # compute predictions / uncertainty for ranking on unlabelled dataset\n",
    "        uncertain_unlabelled, uncertain_map, unlabelled_prediction = uncertainty_for_ranking(model, X_unlabelled)\n",
    "        save_results(uncertain_map, \"uncertainty_map_x_unlabelled_\", session_name, random_bool)\n",
    "\n",
    "\n",
    "        print(\"------------Computing scores on test set -----------\")\n",
    "        # score computation on X_test\n",
    "        score_global = scores(model, X_test, y_test, score_global)\n",
    "        save_results(score_global, \"score_global_\", session_name, random_bool)\n",
    "    \n",
    "    # save score global on test set for each experiment\n",
    "    score_average.append(score_global)\n",
    "    save_results(score_average, \"score_average_\", session_name, random_bool)\n",
    "\n",
    "    \n",
    "    # track selected samples over experiments\n",
    "    indexes_experiments.append(to_be_annotated_index)\n",
    "    print \"Selected samples over experiments=\", indexes_experiments\n",
    "    save_results(indexes_experiments, \"index_uncert_\", session_name, random_bool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Dice on Test Set, compared with Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_average_random = np.load(results_path + \"score_average_\"+ session_name + \"_random.npy\")\n",
    "score_average_uncert = np.load(results_path + \"score_average_\" + session_name + \".npy\")\n",
    "\n",
    "mean_uncert = np.mean(score_average_uncert, axis = 0)\n",
    "mean_rand = np.mean(score_average_random, axis = 0)\n",
    "\n",
    "std_uncert= np.std(score_average_uncert, axis = 0)\n",
    "std_rand = np.std(score_average_random, axis = 0)\n",
    "\n",
    "print\"Random Baseline array (Mean and Std) shapes: \", mean_rand.shape, std_rand.shape\n",
    "print\"Uncertainty method array (Mean and Std) shapes: \", score_average_uncert.shape, score_average_random.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_rand = mean_rand[1:, 0]\n",
    "sigma_rand = std_rand[1:, 0]\n",
    "\n",
    "mu_uncert = mean_uncert[1:, 0]\n",
    "sigma_uncert = std_uncert[1:, 0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 5))\n",
    "ax.plot(range(1, len(mu_rand)+1), mu_rand, lw=2, label='Random', color='blue')\n",
    "ax.plot(range(1, len(mu_uncert)+1), mu_uncert, lw=2, label='Uncert', color='green')\n",
    "ax.fill_between(range(1, len(mu_rand)+1), mu_rand+sigma_rand, mu_rand-sigma_rand, facecolor='blue', alpha=0.1)\n",
    "ax.fill_between(range(1, len(mu_uncert)+1),mu_uncert+sigma_uncert, mu_uncert-sigma_uncert, facecolor='green', alpha=0.1)\n",
    "ax.set_title('Segmentation results on Test Set  - Mean and std over multiple experiments', fontsize = 20)\n",
    "ax.legend(loc='lower right', fontsize = 20)\n",
    "ax.set_xlabel('Iteration', fontsize = 20)\n",
    "ax.set_ylabel('Dice', fontsize = 20)\n",
    "ax.set_xticks(range(1, len(mu_uncert)))\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Segmentation results on Test set for Top 3 MOST UNCERTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uncertain_test, uncertain_map_test, predictions_test = uncertainty_for_ranking(model, X_test)\n",
    "predictions_test = np.asarray(predictions_test)\n",
    "print \"Top 3 most uncertain samples in test set: \" , np.argsort(uncertain_test)[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for indice in np.argsort(uncertain_test)[-3:]:\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.subplot(141)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(predictions_test[indice][:,:])\n",
    "    plt.title(\"Prediction sample \" + str(indice))\n",
    "    plt.subplot(142)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.asarray(uncertain_map_test)[indice, 0])\n",
    "    plt.title(\"Uncertainty map sample \"+ str(indice))\n",
    "    plt.subplot(143)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_test[indice][0,:,:])\n",
    "    plt.title(\"Sample \"+ str(indice))\n",
    "    plt.subplot(144)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y_test[indice][0,:,:])\n",
    "    plt.title(\"Ground Truth sample \"+ str(indice))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Segmentation results on Test set at a specific iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_iter = unet_full_bn(dropout = True)\n",
    "# load weights of your model at a certain iteration\n",
    "model_iter.load_weights('../models/AL_model_test_10.hdf5')\n",
    "uncertain_test, uncertain_map_test, predictions_test = uncertainty_for_ranking(model_iter, X_test)\n",
    "predictions_test = np.asarray(predictions_test)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(221)\n",
    "plt.imshow(predictions_test[2][:,:])\n",
    "plt.title(\"Prediction sample \" + str(2))\n",
    "plt.axis('off')\n",
    "plt.subplot(222)\n",
    "plt.imshow(np.asarray(uncertain_map_test)[2, 0])\n",
    "plt.title(\"Uncertainty map sample \"+ str(2))\n",
    "plt.axis('off')\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_test[2][0,:,:])\n",
    "plt.title(\"Sample \"+ str(2))\n",
    "plt.axis('off')\n",
    "plt.subplot(224)\n",
    "plt.imshow(y_test[2][0,:,:])\n",
    "plt.title(\"Ground Truth sample \"+ str(2))\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dal_venv)",
   "language": "python",
   "name": "dal_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
